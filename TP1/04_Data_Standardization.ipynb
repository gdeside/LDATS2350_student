{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a47fd7-ea9c-4c95-a662-1bbca0f0849a",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue;\">[LDATS2350] - DATA MINING</span>\n",
    "\n",
    "### <span style=\"color:darkred;\">Python04 - Data Standardization</span>\n",
    "\n",
    "**Prof. Robin Van Oirbeek**  \n",
    "\n",
    "<br/>\n",
    "\n",
    "**<span style=\"color:darkgreen;\">Guillaume Deside</span>** (<span style=\"color:gray;\">guillaume.deside@uclouvain.be</span>)\n",
    "\n",
    "---\n",
    "\n",
    "## **What is Data Standardization?**\n",
    "\n",
    "**Data standardization** is a crucial preprocessing step in data mining and machine learning. It involves transforming your data so that each feature has the same scale, typically with a mean of 0 and a standard deviation of 1. This ensures that all variables contribute equally to the analysis and model training.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Standardize Data?**\n",
    "\n",
    "### 1. **Improves Model Performance**\n",
    "Some machine learning algorithms, such as **k-nearest neighbors**, **support vector machines**, and **gradient descent-based models**, are sensitive to the scale of input data. Standardization ensures that features with large magnitudes do not dominate features with smaller magnitudes.\n",
    "\n",
    "### 2. **Enhances Interpretability**\n",
    "Standardized data allows for better interpretability of results, especially in models where coefficients indicate the importance of features.\n",
    "\n",
    "### 3. **Speeds Up Convergence**\n",
    "In optimization-based models (e.g., logistic regression, neural networks), standardizing the data can lead to faster convergence during training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225d339-6362-4d76-bdae-b26825429d34",
   "metadata": {},
   "source": [
    "## Analsing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f026a1-99c1-435b-84bc-e8f3302e43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78d3c2d5-b5a3-448a-8f09-b7ce80838857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5d65a7-2728-4ab2-9d95-6470e2a78941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the dataframe\n",
    "dataset_df = pd.DataFrame(dataset.data)\n",
    "\n",
    "columns = dataset.feature_names\n",
    "dataset_df.columns = columns\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a41779b-43aa-44bd-bec2-b390f28a7093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       mean symmetry  mean fractal dimension  ...  worst radius  \\\n",
       "count     569.000000              569.000000  ...    569.000000   \n",
       "mean        0.181162                0.062798  ...     16.269190   \n",
       "std         0.027414                0.007060  ...      4.833242   \n",
       "min         0.106000                0.049960  ...      7.930000   \n",
       "25%         0.161900                0.057700  ...     13.010000   \n",
       "50%         0.179200                0.061540  ...     14.970000   \n",
       "75%         0.195700                0.066120  ...     18.790000   \n",
       "max         0.304000                0.097440  ...     36.040000   \n",
       "\n",
       "       worst texture  worst perimeter   worst area  worst smoothness  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       worst compactness  worst concavity  worst concave points  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       worst symmetry  worst fractal dimension  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33feec-43b9-44bb-9b1a-2bf7bc5882d0",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid darkblue; padding: 10px; background-color: #89D9F5;\">\n",
    "\n",
    "### **Exercise - Creating and Customizing a Boxplot**\n",
    "\n",
    "#### **Objective**\n",
    "Learn how to create and customize a **boxplot** using Matplotlib to visualize the distribution of data across multiple features.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Instructions**\n",
    "1. Import the necessary libraries:  \n",
    "   - `pandas` for creating a dataset.\n",
    "   - `matplotlib.pyplot` for plotting.\n",
    "\n",
    "2. Create a Pandas DataFrame `dataset_df` with at least **four columns** and **random numeric data**.  \n",
    "   *(Hint: Use `np.random.rand()` to generate random data.)*\n",
    "\n",
    "3. Generate a **basic boxplot** and **basic histograms** of the DataFrame using `dataset_df.boxplot()`.\n",
    "\n",
    "\n",
    "4. Display the final customized boxplot.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74325845-ebd4-4f8a-852e-0f55345a345f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2388c6ad-fee5-4cf5-b1b5-72ed166450f2",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "<img src=\"boxplot_features.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a84157-cb13-4a0b-bc42-bae354b04f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6b4d4f-6d0a-420d-bdbc-4cf2743e91bd",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "<img src=\"histogram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda10444-e4b6-401f-b802-0ae770b032c3",
   "metadata": {},
   "source": [
    "# **Standardization with StandardScaler**\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Standardization?**\n",
    "Standardization is a preprocessing technique that transforms features to have a **mean of 0** and a **standard deviation of 1**. This ensures that all features contribute equally to the analysis, avoiding dominance by features with larger magnitudes.\n",
    "\n",
    "Mathematically, for each feature $X_j$, the standardized value \\( Z_j \\) is calculated as:\n",
    "\n",
    "$\n",
    "Z_j = \\frac{X_j - \\mu_j}{\\sigma_j}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ Z_j $: The standardized value of the feature \\( X_j \\).\n",
    "- $ X_j $: The original feature value.\n",
    "- $ \\mu_j $: The **mean** of the feature \\( X_j \\) (calculated on the training data).\n",
    "- $ \\sigma_j $: The **standard deviation** of the feature \\( X_j \\) (calculated on the training data).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Standardize Data?**\n",
    "1. **Improves Model Performance**:\n",
    "   - Models like **SVM**, **Logistic Regression**, and **KNN** are sensitive to the scale of features.\n",
    "2. **Assumes Normalized Features**:\n",
    "   - Techniques like **PCA** and **LDA** assume the data is normally distributed.\n",
    "3. **Balances Feature Contribution**:\n",
    "   - Prevents features with large magnitudes from dominating those with smaller ones.\n",
    "\n",
    "---\n",
    "\n",
    "### **StandardScaler in Practice**\n",
    "\n",
    "The `StandardScaler` class from `sklearn.preprocessing` is used to perform standardization in Python. It works by:\n",
    "1. **Fitting** on the training data to compute the mean (\\( \\mu_j \\)) and standard deviation (\\( \\sigma_j \\)) for each feature.\n",
    "2. **Transforming** the dataset by centering (subtracting the mean) and scaling (dividing by the standard deviation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddad2084-7f15-4a0d-856b-bd343aaab6e0",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid darkblue; padding: 10px; background-color: #89D9F5;\">\n",
    "\n",
    "### **Exercise: Standardize and Visualize Data with `StandardScaler`**\n",
    "\n",
    "#### **Objective**\n",
    "Learn how to use **`StandardScaler`** from `sklearn.preprocessing` to standardize your dataset and visualize the standardized data using a boxplot.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Instructions**\n",
    "1. **Fit the Scaler**:\n",
    "   - Use `StandardScaler` to compute the mean and standard deviation for the features in the dataset.\n",
    "   \n",
    "2. **Transform the Data**:\n",
    "   - Apply the transformation to standardize the dataset.  \n",
    "\n",
    "3. **Convert to DataFrame**:\n",
    "   - Convert the standardized data back into a Pandas DataFrame with the same column names as the original dataset.\n",
    "\n",
    "4. **Plot the Boxplot**:\n",
    "   - Create a **boxplot** for the standardized data to compare the feature distributions after standardization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Hints**\n",
    "- Use `scaler.fit()` to compute the required statistics (mean and standard deviation).\n",
    "- Use `scaler.transform()` to standardize the dataset.\n",
    "- Use `pd.DataFrame()` to create a new DataFrame from the transformed data.\n",
    "- Use `plt.boxplot()` or Pandas' `boxplot()` method for visualization.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8bb87-beb7-4b5a-b1f5-90daf2885517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869a524f-b994-4071-877c-487e785b3759",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "<img src=\"standardizer.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4447e4-e39d-49b4-9ac9-94083c125f15",
   "metadata": {},
   "source": [
    "## **Using MinMaxScaler**\n",
    "\n",
    "---\n",
    "The `MinMaxScaler` is a data preprocessing technique from `sklearn.preprocessing` used to **normalize features** by scaling them to a specific range, typically [0, 1]. Unlike `StandardScaler`, which standardizes data to have a mean of 0 and unit variance, `MinMaxScaler` rescales the data linearly between a minimum and maximum value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula**\n",
    "The transformation performed by `MinMaxScaler` is given by:\n",
    "\n",
    "$\n",
    "X' = \\frac{X - \\text{X}_{\\text{min}}}{\\text{X}_{\\text{max}} - \\text{X}_{\\text{min}}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ X $: Original feature value.\n",
    "- $ X_{\\text{min}} $: Minimum value of the feature in the dataset.\n",
    "- $X_{\\text{max}}$: Maximum value of the feature in the dataset.\n",
    "- $ X' $: Scaled value between 0 and 1.\n",
    "\n",
    "This transformation scales each feature independently.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use MinMaxScaler?**\n",
    "\n",
    "1. **Normalization for Specific Ranges**:\n",
    "   - MinMaxScaler is particularly useful when you want all features to lie within a specific range (e.g., [0, 1] or [-1, 1]).\n",
    "\n",
    "2. **Avoiding Feature Dominance**:\n",
    "   - Rescales features to ensure no single feature dominates others during computation (e.g., in distance-based algorithms like KNN or clustering).\n",
    "\n",
    "3. **Maintains Distribution Shape**:\n",
    "   - Unlike `StandardScaler`, MinMaxScaler does not distort the shape of the original feature distribution.\n",
    "\n",
    "4. **Best for Bounded Models**:\n",
    "   - Particularly beneficial for machine learning algorithms sensitive to absolute scales, such as **neural networks** or when features need to fit into a specific activation function range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c80752-a67f-4f77-8667-67a583481e2a",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid darkblue; padding: 10px; background-color: #89D9F5;\">\n",
    "\n",
    "### **Exercise - Normalize Data Using MinMaxScaler**\n",
    "\n",
    "#### **Objective**\n",
    "Learn how to use Scikit-Learn's `MinMaxScaler` to normalize data to a specified range, such as **(-1, 1)**. This exercise will teach you how to scale feature values in a DataFrame.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Instructions**\n",
    "\n",
    "1. **Create or Load a Dataset**:\n",
    "   - Create a Pandas DataFrame named `dataset_df` with at least **three columns** and **random numerical data**. *(Hint: Use `np.random.rand()` to generate random numbers.)*\n",
    "   - Alternatively, use a real dataset if available.\n",
    "\n",
    "2. **Initialize MinMaxScaler**:\n",
    "   - Initialize the `MinMaxScaler` with the following parameters:\n",
    "     - `copy=False`: Ensures the scaling is done **in place** without creating a copy.\n",
    "     - `feature_range=(-1, 1)`: Scales all features into the range [-1, 1].\n",
    "\n",
    "3. **Fit and Transform the Dataset**:\n",
    "   - Apply `fit_transform()` to normalize the data.\n",
    "\n",
    "4. **Verify Results**:\n",
    "   - Print the **original data** and the **normalized data**.\n",
    "   - Verify that all feature values now lie within the range [-1, 1].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9178070-a1c2-40fd-ac72-0b5890d9b752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "84c7571d-a357-45b6-befe-8d2069c1acb1",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "Original Dataset:\n",
    "   Feature_1  Feature_2  Feature_3\n",
    "0  37.454012  21.029225  11.118529\n",
    "1  95.071431  68.495493   6.394939\n",
    "2  73.199394  61.622132   7.921446\n",
    "3  59.865848  30.616956   8.663618\n",
    "4  15.601864  29.091248   9.560700\n",
    "5  15.599452  29.170225  12.851760\n",
    "6   5.808361  35.212112   6.996738\n",
    "7  86.617615  46.237822  10.142344\n",
    "8  60.111501  41.597251  10.924146\n",
    "9  70.807258  34.561457   5.464504\n",
    "\n",
    "Normalized Dataset (Range: -1 to 1):\n",
    "   Feature_1  Feature_2  Feature_3\n",
    "0  -0.290958  -1.000000   0.530751\n",
    "1   1.000000   1.000000  -0.748097\n",
    "2   0.509942   0.710390  -0.334816\n",
    "3   0.211195  -0.596019  -0.133883\n",
    "4  -0.780570  -0.660305   0.108990\n",
    "5  -0.780624  -0.656977   1.000000\n",
    "6  -1.000000  -0.402401  -0.585168\n",
    "7   0.810586   0.062169   0.266462\n",
    "8   0.216699  -0.133362   0.478124\n",
    "9   0.456345  -0.429817  -1.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78202447-a0e9-4945-8852-7f4c31d4f624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
