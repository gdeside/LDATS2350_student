{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be29abca-88ee-4166-964b-c07ebdf4dc6a",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue;\">[LDATS2350] - DATA MINING</span>\n",
    "\n",
    "### <span style=\"color:darkred;\">Python15 - Support Vector Machine (SVM)</span>\n",
    "\n",
    "**Prof. Robin Van Oirbeek**  \n",
    "\n",
    "<br/>\n",
    "\n",
    "**<span style=\"color:darkgreen;\">Guillaume Deside</span>** (<span style=\"color:gray;\">guillaume.deside@uclouvain.be</span>)\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ What is a Support Vector Machine (SVM)?**\n",
    "Support Vector Machine (SVM) is a **supervised learning algorithm** used for classification and regression tasks. SVM is particularly effective in high-dimensional spaces and for cases where the number of dimensions exceeds the number of samples.\n",
    "\n",
    "SVM aims to find the **optimal decision boundary** that best separates data into different classes. This boundary is known as the **hyperplane**.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Mathematical Formulation of SVM**\n",
    "The goal of SVM is to maximize the **margin** between different classes while minimizing classification error.\n",
    "\n",
    "### **1Ô∏è‚É£ Hyperplane Equation**\n",
    "A hyperplane in an **n-dimensional space** is defined as:\n",
    "\n",
    "$$ w \\cdot x + b = 0 $$\n",
    "\n",
    "where:\n",
    "- $ w $ is the **weight vector** (normal to the hyperplane),\n",
    "- $ x $ is the **feature vector**,\n",
    "- $ b $ is the **bias term**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Hard Margin SVM (for Linearly Separable Data)**\n",
    "For a dataset **with two classes** labeled as $ y_i \\in \\{-1,1\\} $, we define the constraints:\n",
    "\n",
    "$$ y_i (w \\cdot x_i + b) \\geq 1, \\quad \\forall i $$\n",
    "\n",
    "The objective is to **maximize the margin**, which is equivalent to minimizing:\n",
    "\n",
    "$$ \\frac{1}{2} ||w||^2 $$\n",
    "\n",
    "subject to the constraint above.\n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Soft Margin SVM (for Non-Separable Data)**\n",
    "In real-world datasets, perfect separation may not be possible. To handle misclassification, **slack variables** ($\\xi_i$) are introduced:\n",
    "\n",
    "$$ y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\forall i $$\n",
    "\n",
    "The optimization problem becomes:\n",
    "\n",
    "$$ \\min_{w,b,\\xi} \\frac{1}{2} ||w||^2 + C \\sum_{i=1}^{n} \\xi_i $$\n",
    "\n",
    "where:\n",
    "- $ C $ is the **regularization parameter** that controls the trade-off between maximizing the margin and allowing classification errors.\n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Kernel Trick for Non-Linear Data**\n",
    "SVM can be extended to **non-linearly separable data** using the **kernel trick**. Instead of working in the original feature space, we **map data to a higher-dimensional space** where it becomes linearly separable.\n",
    "\n",
    "\n",
    "![example](https://miro.medium.com/v2/resize:fit:656/0*KGkhQkRwthxW_Txz.png)\n",
    "\n",
    "Common kernels:\n",
    "- **Linear Kernel**: $ K(x_i, x_j) = x_i \\cdot x_j $\n",
    "- **Polynomial Kernel**: $ K(x_i, x_j) = (x_i \\cdot x_j + c)^d $\n",
    "- **Radial Basis Function (RBF) Kernel**:  \n",
    "  $$ K(x_i, x_j) = \\exp\\left(-\\gamma ||x_i - x_j||^2\\right) $$\n",
    "- **Sigmoid Kernel**:  \n",
    "  $$ K(x_i, x_j) = \\tanh(\\alpha x_i \\cdot x_j + c) $$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Advantages of SVM**\n",
    "‚úÖ Works well in **high-dimensional spaces**.  \n",
    "‚úÖ Effective for **small datasets**.  \n",
    "‚úÖ Can model **non-linear relationships** using kernel functions.  \n",
    "‚úÖ Robust to **overfitting**, especially with proper parameter tuning.\n",
    "\n",
    "## **üîπ Disadvantages of SVM**\n",
    "‚ùå Computationally expensive for **large datasets**.  \n",
    "‚ùå Difficult to interpret results compared to decision trees.  \n",
    "‚ùå Choice of kernel and hyperparameters requires careful tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **üîπ Visualizing SVM with Different Kernels**\n",
    "![SVM Kernel Trick](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_svc_001.png)\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2ebf2c-0ae8-4c11-a8b0-9ddab6636e8f",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0bbb900-755b-4b86-9891-adf67b0c2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas as pd\n",
    "data = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e1359cc-5dd4-4689-a6d6-927c412f21da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 8)\n"
     ]
    }
   ],
   "source": [
    "X = data.iloc[:,0:-1]\n",
    "column_names = list(X) \n",
    "y = data.iloc[:,-1] \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#SPLIT DATA INTO TRAIN AND TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,  #X_scaled\n",
    "                                                    test_size =0.30, #by default is 75%-25%\n",
    "                                                    #shuffle is set True by default,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state= 123) #fix random seed for replicability\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1324a-25a1-4272-9255-9f38dafd02c6",
   "metadata": {},
   "source": [
    "# SVM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92116a34-2802-4417-81b5-49d7d409def4",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid darkblue; padding: 10px; background-color: #89D9F5;\">\n",
    "\n",
    "### üìå **Exercise: Hyperparameter Tuning and Evaluation of an SVM Classifier**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **üìù Step 1: Define the SVM Classifier and Hyperparameter Grid**\n",
    "We will tune the following **hyperparameters** (You can remove some elements to reduce the time):\n",
    "- **Kernel**: `linear`, `rbf`, `polynomial`\n",
    "- **Regularization parameter (C)**: Controls trade-off between margin maximization and misclassification.\n",
    "- **Gamma (for RBF kernel)**: Defines how far influence of a single example reaches.\n",
    "- **Degree (for polynomial kernel)**: Degree of the polynomial function.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **üìù Step 2: Evaluate the Best Model**\n",
    "Once the best hyperparameters are selected, **evaluate** the model on the test set.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **üìù Step 3: Compute and Plot the ROC Curve**\n",
    "ROC curve measures model performance for **binary classification problems**. If working with **multi-class**, modify the approach to **One-vs-Rest (OvR).**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **üí° Tasks for You**\n",
    "1. **Modify the parameter grid**:\n",
    "   - Add more values for `C`, `gamma`, and `degree` to see their effects.\n",
    "   - Try different kernels (`sigmoid`, `poly` with higher degrees).\n",
    "   \n",
    "2. **Interpret the Results**:\n",
    "   - Compare **classification reports** for different kernels.\n",
    "   - What kernel works best for this dataset? Why?\n",
    "\n",
    "3. **Extend to Multi-Class Classification**:\n",
    "   - Modify the **ROC curve computation** to support multi-class classification.\n",
    "\n",
    "---\n",
    "\n",
    "üöÄ **Try different datasets and see how SVM performs in various scenarios!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb68d4-6b30-440c-bb18-1d381a06cded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e0c84b0c-820c-43d8-9dcb-5eefc34882e8",
   "metadata": {},
   "source": [
    "*** GRID SEARCH RESULTS ***\n",
    "Best score: 0.767407 using {'C': 1, 'gamma': 0.001, 'kernel': 'linear'}\n",
    "*** Classification Report ***\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.77      0.87      0.82       150\n",
    "           1       0.68      0.53      0.60        81\n",
    "\n",
    "    accuracy                           0.75       231\n",
    "   macro avg       0.73      0.70      0.71       231\n",
    "weighted avg       0.74      0.75      0.74       231"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e8161b-31c7-40ef-a6dc-5810ae93cfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "3774b6c5-befd-4186-948c-d2175e4b6ab7",
   "metadata": {},
   "source": [
    "AUC Score: 0.85\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
