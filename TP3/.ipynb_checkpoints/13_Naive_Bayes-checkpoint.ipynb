{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106ff77e-b505-4547-801a-a19b42446f8c",
   "metadata": {},
   "source": [
    "# <span style=\"color:darkblue;\">[LDATS2350] - DATA MINING</span>\n",
    "\n",
    "### <span style=\"color:darkred;\">Python13 - Na√Øve Bayes</span>\n",
    "\n",
    "**Prof. Robin Van Oirbeek**  \n",
    "\n",
    "<br/>\n",
    "\n",
    "**<span style=\"color:darkgreen;\">Guillaume Deside</span>** (<span style=\"color:gray;\">guillaume.deside@uclouvain.be</span>)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **What is Na√Øve Bayes?**\n",
    "Na√Øve Bayes is a **probabilistic classification algorithm** based on **Bayes' Theorem**. It is widely used for **text classification**, **spam detection**, **sentiment analysis**, and other applications where categorical data plays a crucial role.\n",
    "\n",
    "### **üî¢ Bayes' Theorem**\n",
    "The Na√Øve Bayes classifier is based on **Bayes' Theorem**, which states:\n",
    "\n",
    "\\[\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "\\]\n",
    "\n",
    "where:\n",
    "- \\( P(A|B) \\) is the **posterior probability** (the probability of class \\( A \\) given evidence \\( B \\))\n",
    "- \\( P(B|A) \\) is the **likelihood** (the probability of evidence \\( B \\) given class \\( A \\))\n",
    "- \\( P(A) \\) is the **prior probability** (the probability of class \\( A \\) before considering \\( B \\))\n",
    "- \\( P(B) \\) is the **marginal probability** (the probability of evidence \\( B \\) occurring)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Why \"Na√Øve\"?**\n",
    "The algorithm is called **\"na√Øve\"** because it **assumes** that all features are **independent** given the class label. This assumption **simplifies computation** and works well in many real-world scenarios, despite often being unrealistic.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Types of Na√Øve Bayes Classifiers**\n",
    "Na√Øve Bayes has different variants depending on the data type:\n",
    "\n",
    "1. **Gaussian Na√Øve Bayes (GNB)** üü†  \n",
    "   - Used for **continuous data** (assumes Gaussian distribution).\n",
    "   - Example: **Predicting spam emails based on word frequency**.\n",
    "\n",
    "2. **Multinomial Na√Øve Bayes (MNB)** üü¢  \n",
    "   - Used for **discrete data** (word counts, frequency of events).\n",
    "   - Example: **Text classification (Spam vs. Ham)**.\n",
    "\n",
    "3. **Bernoulli Na√Øve Bayes (BNB)** üîµ  \n",
    "   - Used for **binary feature vectors** (presence or absence of a word).\n",
    "   - Example: **Sentiment Analysis (Positive/Negative review classification)**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Advantages of Na√Øve Bayes**\n",
    "‚úÖ **Fast & Scalable** ‚Äì Works well with large datasets.  \n",
    "‚úÖ **Handles Missing Data** ‚Äì Can still classify even if some features are missing.  \n",
    "‚úÖ **Performs Well with Text Data** ‚Äì Commonly used for NLP tasks.  \n",
    "‚úÖ **Interpretable** ‚Äì Provides a clear probabilistic explanation.  \n",
    "\n",
    "üõë **Limitations**  \n",
    "üöß **Feature Independence Assumption** ‚Äì Often unrealistic in real-world data.  \n",
    "üöß **Zero Probability Issue** ‚Äì If a category never appears in training, the probability becomes 0.  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Na√Øve Bayes in Action**\n",
    "üìå **Spam Detection** ‚Äì Filtering spam emails.  \n",
    "üìå **Sentiment Analysis** ‚Äì Classifying reviews as positive or negative.  \n",
    "üìå **Medical Diagnosis** ‚Äì Predicting diseases based on symptoms.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Key Takeaways**\n",
    "- **Na√Øve Bayes** is a simple yet powerful probabilistic classifier.\n",
    "- **Based on Bayes' Theorem** with an **independence assumption**.\n",
    "- **Works well for text classification and categorical data**.\n",
    "- **Fast, interpretable, and effective** despite its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cce2e-970f-4704-a63c-cf6ebf034853",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78f1dcc-3482-4c11-995a-38bbce03b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4228f0be-ba10-4ff4-a3a3-0948ad48f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:-1]\n",
    "column_names = list(X) \n",
    "y = data.iloc[:,-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f437b9-5dc2-42fa-87ee-02270fcf183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 8)\n",
      "(231, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#SPLIT DATA INTO TRAIN AND TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size =0.30, #by default is 75%-25%\n",
    "                                                    #shuffle is set True by default,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state= 123) #fix random seed for replicability\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0846738-7327-4eae-aa07-ee4e0b97face",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## üìå **Bayes Theorem in Na√Øve Bayes Classification**\n",
    "Bayes' Theorem provides a way to update our beliefs based on new evidence. In the context of classification, it allows us to compute the **posterior probability** of a class given the observed features.\n",
    "\n",
    "### **üîπ Bayes' Theorem Formula**\n",
    "\n",
    "$p(y|\\mathbf{x}) = \\frac{p(\\mathbf{x}|y)p(y)}{p(\\mathbf{x})} = \\frac{p(\\mathbf{x}|y)p(y)}{\\sum_{i=1}^H p(\\mathbf{x}|y_i)p(y_i)}$\n",
    "\n",
    "- $p(y|\\mathbf{x})$ : Posterior probability of class \\( y \\) given the feature vector $\\mathbf{x} .$\n",
    "- $ p(\\mathbf{x}|y) $: Likelihood ‚Äì Probability of observing $\\mathbf{x}$  given class  y .\n",
    "- $ p(y) $: Prior probability of class $ y $ (before seeing the features).\n",
    "- $ p(\\mathbf{x}) $: Normalization factor (marginal probability of $ \\mathbf{x}) .$\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Maximum A Posteriori (MAP) Hypothesis**\n",
    "In classification, we want to predict the **most probable class** given the features. The **Maximum A Posteriori (MAP) hypothesis** selects the class with the highest posterior probability:\n",
    "\n",
    "\n",
    "$y_{MAP} = \\arg \\max_{y \\in H} P(y|\\mathbf{x}) = \\arg \\max \\frac{p(\\mathbf{x}|y)p(y)}{\\sum_{i=1}^H p(\\mathbf{x}|y_i)p(y_i)}$\n",
    "\n",
    "\n",
    "Since the denominator $ p(\\mathbf{x}) $ is constant across all classes, we simplify it as:\n",
    "\n",
    "\n",
    "$y_{MAP} = \\arg \\max_{y \\in H} p(\\mathbf{x}|y)p(y)$\n",
    "\n",
    "This means that we classify a new sample by choosing the class that **maximizes the product of the likelihood and the prior probability**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Na√Øve Independence Assumption**\n",
    "The **\"na√Øve\"** assumption in Na√Øve Bayes states that all features are **conditionally independent** given the class:\n",
    "\n",
    "\n",
    "$P(\\mathbf{x}|y) = P(x_1|y)P(x_2|y)...P(x_n|y)=\\prod_{j=1}^n P(x_j|y)$\n",
    "\n",
    "This simplifies computation significantly since we do not need to compute joint probabilities for multiple features.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ **Handling Different Types of Attributes**\n",
    "### **1Ô∏è‚É£ Categorical/Discrete Attributes**\n",
    "For categorical features (e.g., words in text classification, colors, categories):\n",
    "\n",
    "\n",
    "$P(x_j|y) = P(x_j = r_{jk} | y = v_n)$\n",
    "\n",
    "Where $ P(x_j = r_{jk} | y = v_n) $ represents the **empirical frequency** of the observed class $v_n $.\n",
    "\n",
    "üìù **Example:**  \n",
    "If we are classifying emails as spam or not spam and we have the feature \"contains the word FREE\", we estimate:\n",
    "\n",
    "$P(\\text{\"FREE\"} | \\text{Spam}) = \\frac{\\text{Count of \"FREE\" in spam emails}}{\\text{Total spam emails}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Numerical (Continuous) Attributes**\n",
    "For numerical features (e.g., age, income), we assume a **Gaussian (Normal) distribution**:\n",
    "\n",
    "\n",
    "$P(x_j|y) \\sim N(\\mu_{jh}, \\sigma_{jh}^2)$\n",
    "\n",
    "That is, we model each feature $ x_j $ as following a **Normal distribution** with:\n",
    "- Mean $ \\mu_{jh} $ estimated from the training data.\n",
    "- Variance $ \\sigma_{jh}^2 $ estimated from the training data.\n",
    "\n",
    "üìù **Example:**  \n",
    "If we classify people as \"High Income\" or \"Low Income\" based on their age:\n",
    "\n",
    "$P(\\text{Age} | \\text{High Income}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **Key Takeaways**\n",
    "‚úÖ **Bayes' Theorem** allows us to calculate class probabilities given the features.  \n",
    "‚úÖ **Na√Øve Bayes assumes feature independence**, making it computationally efficient.  \n",
    "‚úÖ **Categorical features use frequency-based probabilities**, while **numerical features assume a Gaussian distribution**.  \n",
    "‚úÖ **Despite the \"na√Øve\" assumption, Na√Øve Bayes performs well in many real-world applications**, especially in text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda201b-ea23-4bbd-b092-fb4bace1ee27",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid darkblue; padding: 10px; background-color: #89D9F5;\">\n",
    "\n",
    "### **üìå Exercise: Implement and Evaluate a Na√Øve Bayes Classifier**\n",
    "In this exercise, you will **train, evaluate, and analyze** a Na√Øve Bayes model for classification.\n",
    "\n",
    "#### **üöÄ Steps to Follow:**\n",
    "1. **Train a Na√Øve Bayes classifier** using the provided dataset.\n",
    "2. **Make predictions** on both the training and test sets.\n",
    "3. **Evaluate the model's performance** using:\n",
    "   - F1-score for both training and test sets.\n",
    "   - ROC curve and AUC score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1ed147-f3d0-4a51-93af-555ef115a3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a1096019-9cee-43a2-ac77-850aeeaabfd7",
   "metadata": {},
   "source": [
    "***RESULTS ON TEST SET***\n",
    "F1-score (Test): 0.6666666666666666\n",
    "***RESULTS ON TRAIN SET***\n",
    "F1-score (Train): 0.6288951841359773\n",
    "\n",
    "AUC Score: 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad272714-bce7-4676-8281-d2608e453437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
